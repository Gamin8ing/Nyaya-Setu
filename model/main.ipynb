{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0afe00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "# load HF token from environment variable HF_TOKEN\n",
    "login(token=os.getenv('HF_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "507e5868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbd0bdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./venv/lib/python3.13/site-packages (from scikit-learn) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./venv/lib/python3.13/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Dependencies ensured.\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "def pip_install(package):\n",
    "    try:\n",
    "        __import__(package.split('==')[0].replace('-','_'))\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "for pkg in ['sentence-transformers','PyPDF2','scikit-learn','tqdm','huggingface_hub']:\n",
    "    pip_install(pkg)\n",
    "print('Dependencies ensured.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9dd8c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Optional, Iterable\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
    "except Exception:\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    CROSS_ENCODER_AVAILABLE = True\n",
    "except Exception:\n",
    "    CROSS_ENCODER_AVAILABLE = False\n",
    "import PyPDF2\n",
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5aac3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years: 76 | HF reasoning: True | Model: Llama-3.3-70B-Instruct\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = Path('archive/supreme_court_judgments')\n",
    "TARGET_CASE_FILENAME = 'Abdulla_Ahmed_vs_Animendra_Kissen_Mitter_on_14_March_1950_1.PDF'\n",
    "YEARS = [y for y in os.listdir(DATA_ROOT) if y.isdigit()]\n",
    "USE_HF_REASONING = True\n",
    "\n",
    "REASONING_MODEL_ID = 'meta-llama/Llama-3.3-70B-Instruct'\n",
    "HF_MAX_TOKENS = 400\n",
    "HF_TEMPERATURE = 0.3\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 120\n",
    "MAX_FILES_PER_YEAR = 20\n",
    "USE_CROSS_ENCODER = False\n",
    "CROSS_ENCODER_MODEL = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
    "PRIORITIZE_SUPREME = True\n",
    "CITATION_BOOST = True\n",
    "\n",
    "PRELIM_TOP_K = 10 \n",
    "FINAL_TOP_K = 5\n",
    "print(f'Years: {len(YEARS)} | HF reasoning: {USE_HF_REASONING} | Model: {REASONING_MODEL_ID.split(\"/\")[-1][:30]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41ae53de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_text(pdf_path: Path) -> str:\n",
    "    text_parts = []\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                try:\n",
    "                    page_text = page.extract_text() or ''\n",
    "                except Exception:\n",
    "                    page_text = ''\n",
    "                text_parts.append(page_text)\n",
    "    except Exception as e:\n",
    "        print(f'Failed to read {pdf_path}: {e}')\n",
    "    text = '\\n'.join(text_parts)\n",
    "\n",
    "    text = re.sub(r'[ \\t\\u00A0]+', ' ', text)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ad1572",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECTION_KEYS = ['case_title','date','court','bench','facts','issues','verdict_reasoning','citation']\n",
    "def extract_sections(text: str) -> Dict[str, str]:\n",
    "\n",
    "    data = {k: '' for k in SECTION_KEYS}\n",
    "    lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
    "    head = '\\n'.join(lines[:50]) if lines else ''\n",
    "    body = '\\n'.join(lines)\n",
    "\n",
    "    title_match = re.search(r'([A-Z][^\\n]{5,100}?)\\s+(v\\.?|vs\\.?|versus)\\s+([^\\n]{3,120})', head, flags=re.IGNORECASE)\n",
    "    if title_match:\n",
    "        data['case_title'] = (title_match.group(1)+' v. '+title_match.group(3)).strip()\n",
    "    else:\n",
    "    \n",
    "        data['case_title'] = lines[0] if lines else ''\n",
    "\n",
    "    date_match = re.search(r'(\\b\\d{1,2}\\s+[A-Za-z]{3,9}\\s+\\d{4}\\b|\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b)', head)\n",
    "    if date_match:\n",
    "        data['date'] = date_match.group(1)\n",
    "\n",
    "    court_match = re.search(r'(SUPREME COURT OF INDIA|HIGH COURT[^\\n]*|IN THE SUPREME COURT OF INDIA)', body, flags=re.IGNORECASE)\n",
    "    if court_match:\n",
    "        data['court'] = court_match.group(1).title()\n",
    "    bench_match = re.search(r'CORAM\\s*[:\\-]\\s*([^\\n]+)', body, flags=re.IGNORECASE)\n",
    "    if bench_match:\n",
    "        data['bench'] = bench_match.group(1).strip()\n",
    "\n",
    "    citation_match = re.search(r'\\b(\\d{4}\\s*\\(\\d+\\)\\s*SCC\\s*\\d+|\\d{4}\\s*SCC\\s*\\(.*?\\)\\s*\\d+|AIR\\s*\\d{4}\\s*SC\\s*\\d+|\\d{4}\\s*SCR\\s*\\d+)\\b', body)\n",
    "    if citation_match:\n",
    "        data['citation'] = citation_match.group(1)\n",
    "\n",
    "    def extract_block(keyword_patterns: List[str]) -> str:\n",
    "\n",
    "        pattern = re.compile('|'.join(keyword_patterns), re.IGNORECASE)\n",
    "        idx = None\n",
    "        for i, line in enumerate(lines):\n",
    "            if pattern.search(line):\n",
    "                idx = i\n",
    "                break\n",
    "        if idx is None:\n",
    "            return ''\n",
    "\n",
    "        block = []\n",
    "        for j in range(idx+1, len(lines)):\n",
    "            line = lines[j]\n",
    "            if re.match(r'^[A-Z][A-Z\\s\\-]{2,60}$', line) and len(line.split()) <= 8:\n",
    "                break\n",
    "            block.append(line)\n",
    "        return '\\n'.join(block).strip()\n",
    "    data['facts'] = extract_block(['FACTS','BACKGROUND','FACTUAL MATRIX']) or '\\n'.join(lines[:200])\n",
    "    data['issues'] = extract_block(['ISSUE','QUESTION(S) OF LAW','POINTS FOR DETERMINATION'])\n",
    "    data['verdict_reasoning'] = extract_block(['HELD','JUDGMENT','REASON','CONCLUSION','ORDER','DECISION'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e0c06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding backend (enhanced) ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class EmbeddingBackend:\n",
    "    def __init__(self, model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        self.mode = 'sbert'\n",
    "        self.model = None\n",
    "        self.vectorizer = None\n",
    "        if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "            try:\n",
    "                self.model = SentenceTransformer(model_name)\n",
    "                self.mode = 'sbert'\n",
    "            except Exception as e:\n",
    "                print(f'Failed to load SBERT model ({e}); falling back to TF-IDF.')\n",
    "                self._init_tfidf()\n",
    "        else:\n",
    "            print('sentence-transformers not available; using TF-IDF fallback.')\n",
    "            self._init_tfidf()\n",
    "    def _init_tfidf(self):\n",
    "        self.mode = 'tfidf'\n",
    "        self.vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=60000)\n",
    "    def fit_corpus(self, docs: List[str]):\n",
    "        if self.mode == 'tfidf':\n",
    "            self.vectorizer.fit(docs)\n",
    "    def encode(self, docs: List[str]) -> np.ndarray:\n",
    "        if self.mode == 'sbert' and self.model is not None:\n",
    "            arr = self.model.encode(docs, normalize_embeddings=True, convert_to_numpy=True, batch_size=32)\n",
    "            return arr\n",
    "        X = self.vectorizer.transform(docs) if hasattr(self.vectorizer, 'vocabulary_') and self.vectorizer.vocabulary_ else self.vectorizer.fit_transform(docs)\n",
    "        from sklearn.preprocessing import normalize\n",
    "        return normalize(X).toarray()\n",
    "def aggregate_chunk_vectors(vectors: np.ndarray, method: str = 'mean') -> np.ndarray:\n",
    "    if method == 'max':\n",
    "        return vectors.max(axis=0, keepdims=True)\n",
    "\n",
    "    return vectors.mean(axis=0, keepdims=True)\n",
    "print('Embedding backend (enhanced) ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b6ad309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def gather_case_texts(limit_per_year: Optional[int]=None) -> List[Tuple[str,str,str]]:\n",
    "    records = []\n",
    "    for year in YEARS:\n",
    "        year_path = DATA_ROOT / year\n",
    "        pdfs = [f for f in os.listdir(year_path) if f.lower().endswith('.pdf')]\n",
    "        if limit_per_year:\n",
    "            pdfs = pdfs[:limit_per_year]\n",
    "        for pdf in pdfs:\n",
    "            path = year_path / pdf\n",
    "            text = read_pdf_text(path)\n",
    "            records.append((year, pdf, text))\n",
    "    return records\n",
    "def build_document_representation(sections: Dict[str,str]) -> str:\n",
    "    ordered = [sections.get(k,'') for k in SECTION_KEYS]\n",
    "    return '\\n\\n'.join([s for s in ordered if s])\n",
    "print('Functions ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86eb6414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-stage ranking pipeline ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_target_case() -> Tuple[Dict[str,str], str, str]:\n",
    "    target_path = DATA_ROOT / '1950' / TARGET_CASE_FILENAME\n",
    "    raw = read_pdf_text(target_path)\n",
    "    sections = extract_sections(raw)\n",
    "    doc = build_document_representation(sections)\n",
    "    return sections, doc, raw\n",
    "def rank_similar(target_doc: str, corpus: List[Tuple[str,str,str]], top_k: int=5, target_raw: str = '', target_sections_arg: Optional[Dict[str,str]] = None) -> List[Dict[str,object]]:\n",
    "    backend = EmbeddingBackend()\n",
    "    target_vec = doc_embed(backend, target_doc)\n",
    "    if target_sections_arg:\n",
    "        target_sections = target_sections_arg\n",
    "    else:\n",
    "        target_sections = extract_sections(target_doc)\n",
    "    target_cites = extract_citations(target_raw) if target_raw else extract_citations(target_doc)\n",
    "    vectors = []\n",
    "    meta = []\n",
    "    texts_for_encode = []\n",
    "    if backend.mode == 'tfidf':\n",
    "        texts_for_encode = [build_document_representation(extract_sections(t)) for (_,_,t) in corpus]\n",
    "        backend.fit_corpus([target_doc] + texts_for_encode)\n",
    "        cand_matrix = backend.encode(texts_for_encode)\n",
    "        target_vec = backend.encode([target_doc])\n",
    "    for idx, (year, filename, text) in enumerate(corpus):\n",
    "        if backend.mode == 'sbert':\n",
    "            doc_text = build_document_representation(extract_sections(text))\n",
    "            vec = doc_embed(backend, doc_text)\n",
    "        else:\n",
    "            vec = cand_matrix[idx:idx+1]\n",
    "        vectors.append(vec)\n",
    "        meta.append((year, filename, text))\n",
    "    if not vectors:\n",
    "        return []\n",
    "    all_vecs = np.vstack(vectors)\n",
    "    sims = cosine_similarity(target_vec, all_vecs)[0]\n",
    "    prelim = list(zip(meta, sims))\n",
    "    boosted = []\n",
    "    for (year, filename, raw_text), base_score in prelim:\n",
    "        secs = extract_sections(raw_text)\n",
    "        new_score = boost_score(base_score, secs, target_cites, raw_text)\n",
    "        boosted.append(((year, filename, raw_text), new_score, base_score))\n",
    "    boosted.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    prelim_count = PRELIM_TOP_K if USE_HF_REASONING else top_k\n",
    "    top = boosted[:prelim_count]\n",
    "    if USE_CROSS_ENCODER and CROSS_ENCODER_AVAILABLE:\n",
    "        try:\n",
    "            cross_model = CrossEncoder(CROSS_ENCODER_MODEL)\n",
    "            pairs = []\n",
    "            for (year, filename, raw_text), boosted_score, base_score in top:\n",
    "                cand_secs = extract_sections(raw_text)\n",
    "                pairs.append((target_sections.get('issues','') or target_doc[:800], cand_secs.get('issues','') or raw_text[:800]))\n",
    "            ce_scores = cross_model.predict(pairs)\n",
    "            new = []\n",
    "            for ((meta_triplet, boosted_score, base_score), ce) in zip(top, ce_scores):\n",
    "                blend = 0.5*boosted_score + 0.5*float(ce)\n",
    "                new.append((meta_triplet, blend, boosted_score, base_score))\n",
    "            new.sort(key=lambda x: x[1], reverse=True)\n",
    "            if new:\n",
    "                ce_vals = [n[1] for n in new]\n",
    "                mn, mx = min(ce_vals), max(ce_vals)\n",
    "                rng = (mx-mn) or 1.0\n",
    "                top_after_ce = []\n",
    "                for (meta_triplet, blend, boosted_score, base_score) in new:\n",
    "                    norm = (blend-mn)/rng\n",
    "                    top_after_ce.append((meta_triplet, norm))\n",
    "                top_candidates = top_after_ce[:prelim_count]\n",
    "            else:\n",
    "                top_candidates = [(m,b) for (m,b,_) in boosted[:prelim_count]]\n",
    "        except Exception as e:\n",
    "            print(f'Cross-encoder failed ({e}); falling back.')\n",
    "            top_candidates = [(m,s) for (m,s,_) in boosted[:prelim_count]]\n",
    "    else:\n",
    "        top_candidates = [(m,s) for (m,s,_) in boosted[:prelim_count]]\n",
    "\n",
    "    cand_data = []\n",
    "    for (year, filename, raw_text), score in top_candidates:\n",
    "        secs = extract_sections(raw_text)\n",
    "        cand_data.append((secs, score, year, filename))\n",
    "    if USE_HF_REASONING:\n",
    "        explanations = batch_generate_explanations(target_sections, [(s,sc) for (s,sc,_,_) in cand_data])\n",
    "    else:\n",
    "        explanations = []\n",
    "        for (secs, score, _, _) in cand_data:\n",
    "            expl = generate_explanation(target_sections, secs, score)\n",
    "            explanations.append(expl)\n",
    "\n",
    "    results = []\n",
    "    for idx, ((secs, score, year, filename), explanation) in enumerate(zip(cand_data, explanations)):\n",
    "        if idx >= top_k:\n",
    "            break\n",
    "        results.append({\n",
    "            'case_name': best_case_name(filename, secs),\n",
    "            'year': year,\n",
    "            'citation': secs.get('citation',''),\n",
    "            'similarity': round(float(score),4),\n",
    "            'reason': explanation\n",
    "        })\n",
    "    return results\n",
    "print('Two-stage ranking pipeline ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54b9c830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target case: Abdulla Ahmed v. Animendra Kissen Mitter on 14 March, 1950\n",
      "Retrieving top 10 candidates, then using LLM for final 5...\n",
      "\n",
      "=== RESULTS ===\n",
      "[\n",
      "  {\n",
      "    \"case_name\": \"Nandkishore Lalbhai Mehta v. New Era Fabrics P.Ltd.& Ors on 8\",\n",
      "    \"year\": \"2015\",\n",
      "    \"citation\": \"2015 (9) SCC 755\",\n",
      "    \"similarity\": 0.8335,\n",
      "    \"reason\": \"The similarity between this case and the target case lies in the context of contract negotiation and the role of agents or brokers in concluding sales agreements. Both cases involve disputes over the terms of a sale and the obligations of the parties involved, with a focus on the authority of agents to act on behalf of their principals. The legal questions surrounding contract formation and the duties of agents are central to both cases.\"\n",
      "  },\n",
      "  {\n",
      "    \"case_name\": \"Thomson Press (India) Ltd v. Nanak Builders & Investrs.P.Ltd &\",\n",
      "    \"year\": \"2013\",\n",
      "    \"citation\": \"2013 (5) SCC 397\",\n",
      "    \"similarity\": 0.8318,\n",
      "    \"reason\": \"This case shares similarities with the target case in its examination of the relationship between principals and agents in the context of property sales, particularly regarding the agent's authority to conclude contracts. Both cases delve into the nuances of contract law, including the obligations of parties and the implications of an agent's actions on the principal's liabilities. The reasoning in both cases hinges on the interpretation of contractual terms and the application of agency principles.\"\n",
      "  },\n",
      "  {\n",
      "    \"case_name\": \"Desh Bandhu Gupta v. N.L.Anand & Rajinder Singh on 17\",\n",
      "    \"year\": \"1993\",\n",
      "    \"citation\": \"1994 SCC (1) 131\",\n",
      "    \"similarity\": 0.8283,\n",
      "    \"reason\": \"The similarity between this case and the target case is rooted in the issues of contract enforcement and the rights of parties in a sale agreement, including the role of agents and the conditions under which a contract can be deemed concluded. Both cases involve disputes over the performance of contractual obligations and the remedies available to parties when these obligations are not met. The legal doctrines applied in both cases relate to contract law and the principles governing agency relationships.\"\n",
      "  },\n",
      "  {\n",
      "    \"case_name\": \"Manzoor Ahmed Margray v. Gulam Hassan Aram & Ors on 5\",\n",
      "    \"year\": \"1999\",\n",
      "    \"citation\": \"1999 (7) SCC 703\",\n",
      "    \"similarity\": 0.8228,\n",
      "    \"reason\": \"This case is similar to the target case in that it involves a dispute over a sale agreement, with a focus on the obligations of the seller to provide a clear title and the buyer's right to specific performance. Both cases explore the legal principles governing contracts for the sale of property, including the requirements for a valid contract and the remedies available when a party fails to perform. The reasoning in both cases turns on the interpretation of contractual terms and the application of principles related to specific performance.\"\n",
      "  },\n",
      "  {\n",
      "    \"case_name\": \"R. C. Chandiok & Anr v. Chuni Lal Sabharwal & Ors on 12\",\n",
      "    \"year\": \"1970\",\n",
      "    \"citation\": \"\",\n",
      "    \"similarity\": 0.8178,\n",
      "    \"reason\": \"The similarity between this case and the target case lies in their shared concern with the principles of contract law, particularly in the context of property sales and the obligations of vendors to provide a clear title. Both cases examine the legal questions surrounding the enforceability of contracts and the conditions under which a party may seek specific performance. The cases also touch on the role of agents and the implications of their actions for the principal's liabilities.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "=== RESULTS ===\n",
      "[\n",
      "  {\n",
      "    \"case_name\": \"Nandkishore Lalbhai Mehta v. New Era Fabrics P.Ltd.& Ors on 8\",\n",
      "    \"year\": \"2015\",\n",
      "    \"citation\": \"2015 (9) SCC 755\",\n",
      "    \"similarity\": 0.8335,\n",
      "    \"reason\": \"The similarity between this case and the target case lies in the context of contract negotiation and the role of agents or brokers in concluding sales agreements. Both cases involve disputes over the terms of a sale and the obligations of the parties involved, with a focus on the authority of agents to act on behalf of their principals. The legal questions surrounding contract formation and the duties of agents are central to both cases.\"\n",
      "  },\n",
      "  {\n",
      "    \"case_name\": \"Thomson Press (India) Ltd v. Nanak Builders & Investrs.P.Ltd &\",\n",
      "    \"year\": \"2013\",\n",
      "    \"citation\": \"2013 (5) SCC 397\",\n",
      "    \"similarity\": 0.8318,\n",
      "    \"reason\": \"This case shares similarities with the target case in its examination of the relationship between principals and agents in the context of property sales, particularly regarding the agent's authority to conclude contracts. Both cases delve into the nuances of contract law, including the obligations of parties and the implications of an agent's actions on the principal's liabilities. The reasoning in both cases hinges on the interpretation of contractual terms and the application of agency principles.\"\n",
      "  },\n",
      "  {\n",
      "    \"case_name\": \"Desh Bandhu Gupta v. N.L.Anand & Rajinder Singh on 17\",\n",
      "    \"year\": \"1993\",\n",
      "    \"citation\": \"1994 SCC (1) 131\",\n",
      "    \"similarity\": 0.8283,\n",
      "    \"reason\": \"The similarity between this case and the target case is rooted in the issues of contract enforcement and the rights of parties in a sale agreement, including the role of agents and the conditions under which a contract can be deemed concluded. Both cases involve disputes over the performance of contractual obligations and the remedies available to parties when these obligations are not met. The legal doctrines applied in both cases relate to contract law and the principles governing agency relationships.\"\n",
      "  },\n",
      "  {\n",
      "    \"case_name\": \"Manzoor Ahmed Margray v. Gulam Hassan Aram & Ors on 5\",\n",
      "    \"year\": \"1999\",\n",
      "    \"citation\": \"1999 (7) SCC 703\",\n",
      "    \"similarity\": 0.8228,\n",
      "    \"reason\": \"This case is similar to the target case in that it involves a dispute over a sale agreement, with a focus on the obligations of the seller to provide a clear title and the buyer's right to specific performance. Both cases explore the legal principles governing contracts for the sale of property, including the requirements for a valid contract and the remedies available when a party fails to perform. The reasoning in both cases turns on the interpretation of contractual terms and the application of principles related to specific performance.\"\n",
      "  },\n",
      "  {\n",
      "    \"case_name\": \"R. C. Chandiok & Anr v. Chuni Lal Sabharwal & Ors on 12\",\n",
      "    \"year\": \"1970\",\n",
      "    \"citation\": \"\",\n",
      "    \"similarity\": 0.8178,\n",
      "    \"reason\": \"The similarity between this case and the target case lies in their shared concern with the principles of contract law, particularly in the context of property sales and the obligations of vendors to provide a clear title. Both cases examine the legal questions surrounding the enforceability of contracts and the conditions under which a party may seek specific performance. The cases also touch on the role of agents and the implications of their actions for the principal's liabilities.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "MAX_FILES_PER_YEAR = 20\n",
    "target_sections, target_doc, target_raw = process_target_case()\n",
    "print('Target case:', target_sections.get('case_title', 'Unknown'))\n",
    "print(f'Retrieving top {PRELIM_TOP_K} candidates, then using LLM for final {FINAL_TOP_K}...')\n",
    "corpus = gather_case_texts(limit_per_year=MAX_FILES_PER_YEAR)\n",
    "corpus = [(y,f,t) for (y,f,t) in corpus if f != TARGET_CASE_FILENAME or y != '1950']\n",
    "results = rank_similar(target_doc, corpus, top_k=FINAL_TOP_K, target_raw=target_raw, target_sections_arg=target_sections)\n",
    "output = [{\n",
    "    'case_name': r['case_name'],\n",
    "    'year': r['year'],\n",
    "    'citation': r['citation'],\n",
    "    'similarity': r['similarity'],\n",
    "    'reason': r['reason']\n",
    "} for r in results]\n",
    "print('\\n=== RESULTS ===')\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f115bdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking utility ready.\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text: str, size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
    "    cleaned = text\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(cleaned):\n",
    "        end = i + size\n",
    "        segment = cleaned[i:end]\n",
    "        if segment.strip():\n",
    "            chunks.append(segment.strip())\n",
    "        i = end - overlap\n",
    "        if i < 0:\n",
    "            break\n",
    "    return chunks\n",
    "print('Chunking utility ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e539b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation utilities and doc_embed ready.\n"
     ]
    }
   ],
   "source": [
    "CITATION_REGEX = re.compile(r\"\\b(\\d{4}\\s*\\(\\d+\\)\\s*SCC\\s*\\d+|\\d{4}\\s*SCC\\s*\\(.*?\\)\\s*\\d+|AIR\\s*\\d{4}\\s*SC\\s*\\d+|\\d{4}\\s*SCR\\s*\\d+)\\b\")\n",
    "def extract_citations(text: str) -> set:\n",
    "    return set(CITATION_REGEX.findall(text))\n",
    "def doc_embed(backend: EmbeddingBackend, text: str) -> np.ndarray:\n",
    "    if backend.mode == 'sbert':\n",
    "        chs = chunk_text(text, CHUNK_SIZE, CHUNK_OVERLAP) or [text]\n",
    "        vecs = backend.encode(chs)\n",
    "        return aggregate_chunk_vectors(vecs, 'mean')\n",
    "\n",
    "    return backend.encode([text])\n",
    "def boost_score(base: float, secs: Dict[str,str], target_cites: set, cand_text: str) -> float:\n",
    "    score = base\n",
    "    if PRIORITIZE_SUPREME and secs.get('court','').lower().find('supreme court') >= 0:\n",
    "        score += 0.02  \n",
    "    if CITATION_BOOST:\n",
    "        cand_cites = extract_citations(cand_text)\n",
    "        if cand_cites & target_cites:\n",
    "            score += 0.02\n",
    "    return float(max(0.0, min(1.0, score)))\n",
    "print('Citation utilities and doc_embed ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "088330ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and key term utilities ready.\n"
     ]
    }
   ],
   "source": [
    "LEGAL_NOISE_PREFIX = re.compile(r'^(BENCH:|CITATION:|ACT:|CORAM:|APPEAL|CRL\\.|S\\.L\\.P\\.|JUDGMENT:?|ORDER:?)', re.IGNORECASE)\n",
    "TOKEN_PATTERN = r'(?u)\\b[a-zA-Z][a-zA-Z\\-]{2,}\\b'\n",
    "def clean_section_text(s: str, max_chars: int = 1200) -> str:\n",
    "    if not s:\n",
    "        return ''\n",
    "    lines = [ln.strip() for ln in s.splitlines() if ln.strip()]\n",
    "    lines = [ln for ln in lines if not LEGAL_NOISE_PREFIX.match(ln)]\n",
    "    text = ' '.join(lines)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_chars]\n",
    "def get_key_terms(a: str, b: str, top_k: int = 12) -> Tuple[List[str], List[str], List[str]]:\n",
    "    vec = TfidfVectorizer(stop_words='english', ngram_range=(1,2), token_pattern=TOKEN_PATTERN, min_df=1, max_features=4000)\n",
    "    X = vec.fit_transform([a, b])\n",
    "    terms = np.array(vec.get_feature_names_out())\n",
    "    a_scores = X.toarray()[0]\n",
    "    b_scores = X.toarray()[1]\n",
    "    a_top_idx = np.argsort(a_scores)[::-1][:top_k]\n",
    "    b_top_idx = np.argsort(b_scores)[::-1][:top_k]\n",
    "    a_top = [t for t,s in zip(terms[a_top_idx], a_scores[a_top_idx]) if s>0][:top_k]\n",
    "    b_top = [t for t,s in zip(terms[b_top_idx], b_scores[b_top_idx]) if s>0][:top_k]\n",
    "    overlap = list(dict.fromkeys([t for t in a_top if t in set(b_top)]) )\n",
    "    return a_top, b_top, overlap\n",
    "def best_case_name(filename: str, sections: Dict[str,str]) -> str:\n",
    "    if sections.get('case_title'):\n",
    "        return sections['case_title'][:160]\n",
    "    base = os.path.splitext(filename)[0].replace('_',' ')\n",
    "    # Trim trailing ' on <date>' if present\n",
    "    base = re.split(r'\\s+on\\s+\\d', base, maxsplit=1)[0]\n",
    "    return base[:160]\n",
    "print('Cleaning and key term utilities ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea13366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch LLM reasoning (chat API) ready.\n"
     ]
    }
   ],
   "source": [
    "def batch_generate_explanations(target_sections: Dict[str,str], candidates: List[Tuple[Dict[str,str], float]]) -> List[str]:\n",
    "    if not USE_HF_REASONING or not candidates:\n",
    "        return ['No explanation available.' for _ in candidates]\n",
    "    try:\n",
    "        client = InferenceClient()\n",
    "    except Exception as e:\n",
    "        return [f'HF client init failed: {e}' for _ in candidates]\n",
    "    t_title = target_sections.get('case_title','Unknown')\n",
    "    t_date = target_sections.get('date','')\n",
    "    t_court = target_sections.get('court','')\n",
    "    t_issues = clean_section_text(target_sections.get('issues',''))[:800]\n",
    "    t_reasoning = clean_section_text(target_sections.get('verdict_reasoning',''))[:800]\n",
    "    cand_blocks = []\n",
    "    for idx, (cand_sec, score) in enumerate(candidates, 1):\n",
    "        c_title = cand_sec.get('case_title','Unknown')\n",
    "        c_date = cand_sec.get('date','')\n",
    "        c_court = cand_sec.get('court','')\n",
    "        c_issues = clean_section_text(cand_sec.get('issues',''))[:700]\n",
    "        c_reasoning = clean_section_text(cand_sec.get('verdict_reasoning',''))[:700]\n",
    "        cand_blocks.append(f\"\"\"CANDIDATE {idx} (Similarity: {score:.3f}):\n",
    "Title: {c_title}\n",
    "Date: {c_date}\n",
    "Court: {c_court}\n",
    "Issues: {c_issues}\n",
    "Reasoning: {c_reasoning}\"\"\")\n",
    "    all_cands = '\\n\\n'.join(cand_blocks)\n",
    "    system_msg = \"You are an expert legal AI assistant analyzing Indian court judgments.\"\n",
    "    user_msg = f\"\"\"Compare the TARGET case with each CANDIDATE case below and explain why they are similar in 2-3 precise sentences per candidate.\n",
    "\n",
    "TARGET CASE:\n",
    "Title: {t_title}\n",
    "Date: {t_date}\n",
    "Court: {t_court}\n",
    "Issues: {t_issues}\n",
    "Reasoning: {t_reasoning}\n",
    "\n",
    "{all_cands}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- For each candidate, write exactly 2-3 sentences explaining the similarity.\n",
    "- Focus on: shared factual background, overlapping legal questions, similar reasoning or doctrines applied.\n",
    "- Be concrete and specific; avoid vague phrases like 'both deal with law' or 'similar structure'.\n",
    "- Do not mention bench names, judge panels, or raw citation strings unless critical.\n",
    "- Output format: Start each explanation with 'CANDIDATE X:' on its own line, then your 2-3 sentences.\n",
    "\n",
    "Begin:\"\"\"\n",
    "    try:\n",
    "        resp = client.chat_completion(\n",
    "            messages=[\n",
    "                {'role':'system','content':system_msg},\n",
    "                {'role':'user','content':user_msg}\n",
    "            ],\n",
    "            model=REASONING_MODEL_ID,\n",
    "            max_tokens=HF_MAX_TOKENS*len(candidates),\n",
    "            temperature=HF_TEMPERATURE\n",
    "        )\n",
    "        text = resp.choices[0].message.content.strip()\n",
    "        explanations = []\n",
    "        lines = text.split('\\n')\n",
    "        current = []\n",
    "        for line in lines:\n",
    "            if re.match(r'^CANDIDATE \\d+:', line, re.IGNORECASE):\n",
    "                if current:\n",
    "                    explanations.append(' '.join(current).strip())\n",
    "                current = []\n",
    "            else:\n",
    "                if line.strip():\n",
    "                    current.append(line.strip())\n",
    "        if current:\n",
    "            explanations.append(' '.join(current).strip())\n",
    "        while len(explanations) < len(candidates):\n",
    "            explanations.append('Similarity explanation unavailable.')\n",
    "        return explanations[:len(candidates)]\n",
    "    except Exception as e:\n",
    "        return [f'LLM reasoning failed: {str(e)[:200]}' for _ in candidates]\n",
    "print('Batch LLM reasoning (chat API) ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d4d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Architecture:**\n",
    "1. **Stage 1 (Retrieval):** Use SBERT embeddings + cosine similarity to retrieve top 10 candidate cases\n",
    "2. **Stage 2 (Reasoning):** Send target + all 10 candidates to a powerful LLM (Llama-3.3-70B) to generate precise 2-3 sentence explanations for each, then return top 5\n",
    "\n",
    "**Key Improvements:**\n",
    "- Batch LLM reasoning with full context (target + all candidates in one prompt)\n",
    "- Uses chat completion API for better instruction following\n",
    "- Concrete explanations focusing on factual overlap, legal questions, and doctrinal reasoning\n",
    "- No more generic previews or bench/citation noise\n",
    "\n",
    "**Configuration (Cell 6):**\n",
    "- `REASONING_MODEL_ID`: Switch between meta-llama, Qwen, Mixtral, or others\n",
    "- `PRELIM_TOP_K`: Number of candidates to retrieve (default 10)\n",
    "- `FINAL_TOP_K`: Number with LLM explanations to return (default 5)\n",
    "\n",
    "**To scale up:** Increase `MAX_FILES_PER_YEAR` in runner cell or remove the limit to index the full archive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baded29",
   "metadata": {},
   "source": [
    "# Interactive Legal Chatbot\n",
    "Chat with an AI assistant about your similar cases. The chatbot has full context of the target case and all similar cases found, and can answer questions, compare cases, analyze legal principles, and provide insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c380364c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot configured with model: meta-llama/Llama-3.3-70B-Instruct\n"
     ]
    }
   ],
   "source": [
    "CHATBOT_MODEL_ID = 'meta-llama/Llama-3.3-70B-Instruct'\n",
    "CHATBOT_MAX_TOKENS = 800\n",
    "CHATBOT_TEMPERATURE = 0.4\n",
    "MAX_CONVERSATION_HISTORY = 10\n",
    "\n",
    "print(f'Chatbot configured with model: {CHATBOT_MODEL_ID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415f6bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context builder ready.\n"
     ]
    }
   ],
   "source": [
    "def build_chatbot_context(target_sections: Dict[str,str], similar_cases: List[Dict[str,object]]) -> str:\n",
    "    \"\"\"Create a rich knowledge base from target and similar cases for the chatbot.\"\"\"\n",
    "    \n",
    "    context_parts = [\"=== TARGET CASE ===\"]\n",
    "    context_parts.append(f\"Title: {target_sections.get('case_title', 'Unknown')}\")\n",
    "    context_parts.append(f\"Date: {target_sections.get('date', 'Unknown')}\")\n",
    "    context_parts.append(f\"Court: {target_sections.get('court', 'Unknown')}\")\n",
    "    context_parts.append(f\"Citation: {target_sections.get('citation', 'N/A')}\")\n",
    "    context_parts.append(f\"\\nFacts:\\n{clean_section_text(target_sections.get('facts', ''), 1000)}\")\n",
    "    context_parts.append(f\"\\nIssues:\\n{clean_section_text(target_sections.get('issues', ''), 800)}\")\n",
    "    context_parts.append(f\"\\nReasoning/Verdict:\\n{clean_section_text(target_sections.get('verdict_reasoning', ''), 1000)}\")\n",
    "    \n",
    "    context_parts.append(\"\\n\\n=== SIMILAR CASES ===\")\n",
    "    for idx, case in enumerate(similar_cases, 1):\n",
    "        context_parts.append(f\"\\n--- Case {idx} ---\")\n",
    "        context_parts.append(f\"Name: {case['case_name']}\")\n",
    "        context_parts.append(f\"Year: {case['year']}\")\n",
    "        context_parts.append(f\"Citation: {case.get('citation', 'N/A')}\")\n",
    "        context_parts.append(f\"Similarity Score: {case['similarity']}\")\n",
    "        context_parts.append(f\"Why Similar: {case['reason']}\")\n",
    "    \n",
    "    return '\\n'.join(context_parts)\n",
    "\n",
    "print('Context builder ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c60121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LegalChatbot class ready.\n"
     ]
    }
   ],
   "source": [
    "class LegalChatbot:\n",
    "    \"\"\"Interactive chatbot for discussing legal cases with context awareness.\"\"\"\n",
    "    \n",
    "    def __init__(self, target_sections: Dict[str,str], similar_cases: List[Dict[str,object]], \n",
    "                 model_id: str = CHATBOT_MODEL_ID):\n",
    "        self.target_sections = target_sections\n",
    "        self.similar_cases = similar_cases\n",
    "        self.model_id = model_id\n",
    "        self.conversation_history = []\n",
    "        self.context = build_chatbot_context(target_sections, similar_cases)\n",
    "        \n",
    "        try:\n",
    "            self.client = InferenceClient()\n",
    "            self.available = True\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not initialize HF client: {e}\")\n",
    "            self.available = False\n",
    "    \n",
    "    def chat(self, user_message: str) -> str:\n",
    "        \"\"\"Send a message and get a response from the chatbot.\"\"\"\n",
    "        if not self.available:\n",
    "            return \"Chatbot is not available. Please check your Hugging Face token and internet connection.\"\n",
    "        \n",
    "        if not user_message.strip():\n",
    "            return \"Please enter a question or message.\"\n",
    "        \n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({'role': 'user', 'content': user_message})\n",
    "        \n",
    "        # Keep only recent history to avoid token limits\n",
    "        if len(self.conversation_history) > MAX_CONVERSATION_HISTORY * 2:\n",
    "            self.conversation_history = self.conversation_history[-(MAX_CONVERSATION_HISTORY * 2):]\n",
    "        \n",
    "        # Build system prompt with case context\n",
    "        system_prompt = f\"\"\"You are an expert legal AI assistant helping lawyers and paralegals analyze Indian court judgments.\n",
    "\n",
    "You have access to:\n",
    "1. A TARGET case that the user is researching\n",
    "2. Several SIMILAR cases that were found to be relevant\n",
    "\n",
    "KNOWLEDGE BASE:\n",
    "{self.context}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer questions about the target case, similar cases, or comparisons between them\n",
    "- Provide precise legal analysis based on the facts, issues, and reasoning in the cases\n",
    "- When comparing cases, highlight specific similarities and differences\n",
    "- Cite case names and citations when referencing specific cases\n",
    "- If asked about legal principles, extract them from the reasoning sections\n",
    "- Be concise but thorough; avoid speculation beyond what's in the case documents\n",
    "- If you don't have enough information to answer, say so clearly\n",
    "- Use professional legal language appropriate for lawyers/paralegals\n",
    "\n",
    "Remember: You are analyzing real court judgments. Be accurate and professional.\"\"\"\n",
    "\n",
    "        # Build messages for API\n",
    "        messages = [{'role': 'system', 'content': system_prompt}]\n",
    "        messages.extend(self.conversation_history)\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat_completion(\n",
    "                messages=messages,\n",
    "                model=self.model_id,\n",
    "                max_tokens=CHATBOT_MAX_TOKENS,\n",
    "                temperature=CHATBOT_TEMPERATURE\n",
    "            )\n",
    "            \n",
    "            assistant_message = response.choices[0].message.content.strip()\n",
    "            \n",
    "            # Add assistant response to history\n",
    "            self.conversation_history.append({'role': 'assistant', 'content': assistant_message})\n",
    "            \n",
    "            return assistant_message\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error communicating with chatbot: {str(e)[:300]}\"\n",
    "            return error_msg\n",
    "    \n",
    "    def reset_conversation(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.conversation_history = []\n",
    "        return \"Conversation history cleared. Starting fresh!\"\n",
    "    \n",
    "    def get_suggested_questions(self) -> List[str]:\n",
    "        \"\"\"Return sample questions users might ask.\"\"\"\n",
    "        return [\n",
    "            \"What are the key legal issues in the target case?\",\n",
    "            \"Compare the reasoning in the target case with the most similar case.\",\n",
    "            \"What legal principles are common across all these cases?\",\n",
    "            \"Which case has the strongest precedential value and why?\",\n",
    "            \"Summarize the facts of the target case.\",\n",
    "            \"Are there any conflicting interpretations across these cases?\",\n",
    "            \"What arguments could I make using these similar cases?\",\n",
    "            \"What are the main differences between case 1 and case 2?\",\n",
    "            \"Which cases involve constitutional interpretation?\",\n",
    "            \"What remedies were granted in these cases?\"\n",
    "        ]\n",
    "\n",
    "print('LegalChatbot class ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c89495e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Chatbot initialized successfully!\n",
      "‚úì Target case: Abdulla Ahmed v. Animendra Kissen Mitter on 14 March, 1950\n",
      "‚úì Number of similar cases loaded: 5\n",
      "\n",
      "============================================================\n",
      "CHATBOT READY - You can now ask questions!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize the chatbot with your results\n",
    "chatbot = LegalChatbot(target_sections, results)\n",
    "print(\"‚úì Chatbot initialized successfully!\")\n",
    "print(f\"‚úì Target case: {target_sections.get('case_title', 'Unknown')}\")\n",
    "print(f\"‚úì Number of similar cases loaded: {len(results)}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHATBOT READY - You can now ask questions!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4ce3b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° SUGGESTED QUESTIONS:\n",
      "------------------------------------------------------------\n",
      "1. What are the key legal issues in the target case?\n",
      "2. Compare the reasoning in the target case with the most similar case.\n",
      "3. What legal principles are common across all these cases?\n",
      "4. Which case has the strongest precedential value and why?\n",
      "5. Summarize the facts of the target case.\n",
      "6. Are there any conflicting interpretations across these cases?\n",
      "7. What arguments could I make using these similar cases?\n",
      "8. What are the main differences between case 1 and case 2?\n",
      "9. Which cases involve constitutional interpretation?\n",
      "10. What remedies were granted in these cases?\n"
     ]
    }
   ],
   "source": [
    "# Display suggested questions\n",
    "print(\"üí° SUGGESTED QUESTIONS:\")\n",
    "print(\"-\" * 60)\n",
    "for i, question in enumerate(chatbot.get_suggested_questions(), 1):\n",
    "    print(f\"{i}. {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d71855e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßë‚Äçüíº USER: What are the key legal issues in the target case?\n",
      "\n",
      "ü§ñ CHATBOT:\n",
      "The key legal issues in the target case, Abdulla Ahmed v. Animendra Kissen Mitter (1950 SCR 30), are:\n",
      "\n",
      "1. **Scope of Authority of an Estate Broker**: The court examined the extent of the authority granted to the appellant (estate broker) by the respondent (principal) to negotiate the sale of the property. The commission letter authorized the appellant to \"negotiate the sale\" of the premises, and the court had to determine whether this authority empowered the broker to conclude a contract on behalf of the principal.\n",
      "\n",
      "2. **Construction of Contractual Terms**: The court interpreted the terms of the commission letter and the subsequent correspondence between the parties to ascertain the intentions of the principal and the broker. The court considered whether the broker's actions, including finding a ready and willing purchaser and concluding a contract, were within the scope of his authority.\n",
      "\n",
      "3. **Broker's Right to Commission**: The court addressed the issue of whether the broker was entitled to a commission, despite concluding a contract with the purchaser for a lower price than initially agreed upon. The court's decision turned on the interpretation of the contractual terms and the application of principles related to agency and contract law.\n",
      "\n",
      "4. **Power of Agents and Principals**: The case involved an analysis of the relationship between principals and agents, particularly in the context of contractual negotiations. The court's reasoning considered the scope of an agent's authority and the implications of their actions on the principal's obligations.\n",
      "\n",
      "These legal issues are central to the target case and are relevant to the court's decision, which ultimately turned on the construction of the contractual terms and the application of principles related to agency and contract law.\n"
     ]
    }
   ],
   "source": [
    "# Example: Ask a question\n",
    "question = \"What are the key legal issues in the target case?\"\n",
    "print(f\"\\nüßë‚Äçüíº USER: {question}\\n\")\n",
    "response = chatbot.chat(question)\n",
    "print(f\"ü§ñ CHATBOT:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee307279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INTERACTIVE CHAT MODE\n",
      "============================================================\n",
      "Type your questions below. Type 'exit', 'quit', or 'bye' to stop.\n",
      "Type 'reset' to clear conversation history.\n",
      "Type 'suggestions' to see suggested questions.\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Interactive chat loop - Run this cell and keep asking questions!\n",
    "# Type 'exit', 'quit', or 'bye' to stop\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERACTIVE CHAT MODE\")\n",
    "print(\"=\"*60)\n",
    "print(\"Type your questions below. Type 'exit', 'quit', or 'bye' to stop.\")\n",
    "print(\"Type 'reset' to clear conversation history.\")\n",
    "print(\"Type 'suggestions' to see suggested questions.\")\n",
    "print(\"-\"*60 + \"\\n\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"üßë‚Äçüíº YOU: \").strip()\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "            \n",
    "        if user_input.lower() in ['exit', 'quit', 'bye', 'stop']:\n",
    "            print(\"\\nüëã Chatbot session ended. Thank you!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() == 'reset':\n",
    "            print(f\"\\nü§ñ CHATBOT: {chatbot.reset_conversation()}\\n\")\n",
    "            continue\n",
    "        \n",
    "        if user_input.lower() in ['suggestions', 'suggest', 'help']:\n",
    "            print(\"\\nüí° SUGGESTED QUESTIONS:\")\n",
    "            for i, q in enumerate(chatbot.get_suggested_questions(), 1):\n",
    "                print(f\"   {i}. {q}\")\n",
    "            print()\n",
    "            continue\n",
    "        \n",
    "        response = chatbot.chat(user_input)\n",
    "        print(f\"\\nü§ñ CHATBOT:\\n{response}\\n\")\n",
    "        print(\"-\"*60 + \"\\n\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nüëã Chatbot session interrupted. Goodbye!\")\n",
    "        break\n",
    "    except EOFError:\n",
    "        print(\"\\n\\nüëã Chatbot session ended. Goodbye!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Error: {e}\\n\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb3d63",
   "metadata": {},
   "source": [
    "## Alternative: Single Question Mode\n",
    "If you prefer to ask questions one at a time (without the interactive loop), use the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c380d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a single question - modify and run this cell multiple times\n",
    "my_question = \"Compare the reasoning in the target case with the most similar case.\"\n",
    "\n",
    "print(f\"üßë‚Äçüíº USER: {my_question}\\n\")\n",
    "response = chatbot.chat(my_question)\n",
    "print(f\"ü§ñ CHATBOT:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View conversation history\n",
    "print(\"üìú CONVERSATION HISTORY:\")\n",
    "print(\"=\"*60)\n",
    "for i, msg in enumerate(chatbot.conversation_history):\n",
    "    role = \"üßë‚Äçüíº USER\" if msg['role'] == 'user' else \"ü§ñ CHATBOT\"\n",
    "    content = msg['content'][:200] + \"...\" if len(msg['content']) > 200 else msg['content']\n",
    "    print(f\"\\n{role}:\\n{content}\\n\")\n",
    "    print(\"-\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
